{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","anaconda-cloud":{},"colab":{"name":"MP4_P2_generation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_oZOpUId8URU"},"source":["# Generating Text with an RNN"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OtZaxEId8cYg","executionInfo":{"status":"ok","timestamp":1619583871528,"user_tz":300,"elapsed":16848,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}},"outputId":"7bc4ba0d-1885-45e8-c270-7c6bf943ccbe"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_Z988Ovl8eAv","executionInfo":{"status":"ok","timestamp":1619583871529,"user_tz":300,"elapsed":16846,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}}},"source":["#!chmod +x \"/content/drive/MyDrive/Colab Notebooks/assignment4_materials/assignment4_materials/download_language.sh\"\n","#!\"/content/drive/MyDrive/Colab Notebooks/assignment4_materials/assignment4_materials/download_language.sh\""],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qfdf3XJq8e6f","executionInfo":{"status":"ok","timestamp":1619583871529,"user_tz":300,"elapsed":16845,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}}},"source":["import sys\n","#sys.path.insert(0,\"/CS498DL/assignment4_materials/assignment4_materials/gan\")\n","sys.path.insert(0,\"/content/drive/MyDrive/Colab Notebooks/assignment4_materials/assignment4_materials\")\n","sys.path.insert(0,\"/content/drive/MyDrive/Colab Notebooks/assignment4_materials/assignment4_materials/language_data\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-ION0LS8gOn","executionInfo":{"status":"ok","timestamp":1619583874781,"user_tz":300,"elapsed":20054,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}},"outputId":"31459ba4-0aca-4c40-ff5a-52da31d5b676"},"source":["!pip install unidecode"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n","\r\u001b[K     |█▍                              | 10kB 21.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 29.2MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 20.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 19.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 153kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 163kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 194kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 204kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 215kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 225kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 15.9MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"4mfPNjuR8URZ","executionInfo":{"status":"ok","timestamp":1619583880511,"user_tz":300,"elapsed":25778,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}}},"source":["import unidecode\n","import string\n","import random\n","import re\n","import time\n","\n","import torch\n","import torch.nn as nn\n","\n","%matplotlib inline\n","\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"87PkxZIT8URa","executionInfo":{"status":"ok","timestamp":1619594359781,"user_tz":300,"elapsed":198,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}}},"source":["from rnn.model import RNN\n","from rnn.helpers import time_since\n","from rnn.generate import generate"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"4Y0nP4bf8URa","scrolled":true,"executionInfo":{"status":"ok","timestamp":1619583881493,"user_tz":300,"elapsed":26758,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BrNUWqoV8URa"},"source":["## Data Processing\n","\n","The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"LuPiSXFW8URb","executionInfo":{"status":"ok","timestamp":1619583881927,"user_tz":300,"elapsed":27184,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}},"outputId":"039e3cac-5768-45f8-8b14-86ec5ac92acc"},"source":["all_characters = string.printable\n","n_characters = len(all_characters)\n","\n","file_path = '/content/drive/MyDrive/Colab Notebooks/assignment4_materials/assignment4_materials/language_data/shakespeare.txt'\n","file = unidecode.unidecode(open(file_path).read())\n","file_len = len(file)\n","print('file_len =', file_len)\n","\n","# we will leave the last 1/10th of text as test\n","split = int(0.9*file_len)\n","train_text = file[:split]\n","test_text = file[split:]\n","\n","print('train len: ', len(train_text))\n","print('test len: ', len(test_text))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["file_len = 1115394\n","train len:  1003854\n","test len:  111540\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Yj6kt7p88URb","executionInfo":{"status":"ok","timestamp":1619583881927,"user_tz":300,"elapsed":27178,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}},"outputId":"cd066472-2b47-417a-bc99-db5da5911843"},"source":["chunk_len = 200\n","\n","def random_chunk(text):\n","    start_index = random.randint(0, len(text) - chunk_len)\n","    end_index = start_index + chunk_len + 1\n","    return text[start_index:end_index]\n","\n","print(random_chunk(train_text))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["s?\n","Kissing with inside lip? stopping the career\n","Of laughing with a sigh?--a note infallible\n","Of breaking honesty--horsing foot on foot?\n","Skulking in corners? wishing clocks more swift?\n","Hours, minutes? no\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pu3Jn1Nk8URb"},"source":["### Input and Target data"]},{"cell_type":"markdown","metadata":{"id":"rYj0LCqx8URb"},"source":["To make training samples out of the large string of text data, we will be splitting the text into chunks.\n","\n","Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"U40uSB3z8URc","executionInfo":{"status":"ok","timestamp":1619583881928,"user_tz":300,"elapsed":27178,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}}},"source":["# Turn string into list of longs\n","def char_tensor(string):\n","    tensor = torch.zeros(len(string), requires_grad=True).long()\n","    for c in range(len(string)):\n","        tensor[c] = all_characters.index(string[c])\n","    return tensor"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fMjEo9bq8URc"},"source":["The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"EFHjuBHo8URc","executionInfo":{"status":"ok","timestamp":1619583881928,"user_tz":300,"elapsed":27177,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}}},"source":["def load_random_batch(text, chunk_len, batch_size):\n","    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n","    target = torch.zeros(batch_size, chunk_len).long().to(device)\n","    for i in range(batch_size):\n","        start_index = random.randint(0, len(text) - chunk_len - 1)\n","        end_index = start_index + chunk_len + 1\n","        chunk = text[start_index:end_index]\n","        input_data[i] = char_tensor(chunk[:-1])\n","        target[i] = char_tensor(chunk[1:])\n","    return input_data, target"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y0Tgpmbw8URd"},"source":["# Implement model\n","\n","Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n","\n","\n","You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n","\n","\n","**TODO:** Implement the model in RNN `rnn/model.py`"]},{"cell_type":"markdown","metadata":{"id":"x0hx7tv48URd"},"source":["# Evaluating\n","\n","To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n","\n","\n","Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n","\n","You may check different temperature values yourself, but we have provided a default which should work well."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Gh1Vgh2z8URd","executionInfo":{"status":"ok","timestamp":1619583881928,"user_tz":300,"elapsed":27176,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}}},"source":["def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n","    hidden = rnn.init_hidden(1, device=device)\n","    prime_input = char_tensor(prime_str)\n","    predicted = prime_str\n","\n","    # Use priming string to \"build up\" hidden state\n","    for p in range(len(prime_str) - 1):\n","        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n","    inp = prime_input[-1]\n","    \n","    for p in range(predict_len):\n","        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n","        \n","        # Sample from the network as a multinomial distribution\n","        output_dist = output.data.view(-1).div(temperature).exp()\n","        top_i = torch.multinomial(output_dist, 1)[0]\n","        \n","        # Add predicted character to string and use as next input\n","        predicted_char = all_characters[top_i]\n","        predicted += predicted_char\n","        inp = char_tensor(predicted_char)\n","\n","    return predicted"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WxAfin0c8URe"},"source":["# Train RNN"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"lc_WdPCv8URe","executionInfo":{"status":"ok","timestamp":1619583882034,"user_tz":300,"elapsed":27280,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}}},"source":["batch_size = 100\n","n_epochs = 5000\n","hidden_size = 100\n","n_layers = 1\n","learning_rate = 0.01\n","model_type = 'rnn'\n","print_every = 50\n","plot_every = 50\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"IMLQseab8URe","executionInfo":{"status":"ok","timestamp":1619583882034,"user_tz":300,"elapsed":27279,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}}},"source":["def eval_test(rnn, inp, target):\n","    with torch.no_grad():\n","        hidden = rnn.init_hidden(batch_size, device=device)\n","        loss = 0\n","        for c in range(chunk_len):\n","            output, hidden = rnn(inp[:,c], hidden)\n","            loss += criterion(output.view(batch_size, -1), target[:,c])\n","    \n","    return loss.data.item() / chunk_len"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tItDjp6q8URe"},"source":["### Train function\n","\n","**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"bprNRjXc8URf","executionInfo":{"status":"ok","timestamp":1619583882034,"user_tz":300,"elapsed":27278,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}}},"source":["def train(rnn, input, target, optimizer, criterion):\n","    \"\"\"\n","    Inputs:\n","    - rnn: model\n","    - input: input character data tensor of shape (batch_size, chunk_len)\n","    - target: target character data tensor of shape (batch_size, chunk_len)\n","    - optimizer: rnn model optimizer\n","    - criterion: loss function\n","    \n","    Returns:\n","    - loss: computed loss value as python float\n","    \"\"\"\n","    loss = 0\n","    \n","    ####################################\n","    #          YOUR CODE HERE          #\n","    ####################################\n","    h = rnn.init_hidden(batch_size, device)\n","    rnn.zero_grad()\n","    for c in range(chunk_len):\n","        char = input[:, c]\n","        output, h = rnn.forward(char, h)\n","        \n","        loss += criterion(output.view(batch_size, -1), target[:, c])\n","    loss = loss / chunk_len\n","    loss.backward()\n","    optimizer.step()\n","\n","    ##########       END      ##########\n","\n","    return loss\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Y58aibao8URf","executionInfo":{"status":"ok","timestamp":1619589516983,"user_tz":300,"elapsed":2167577,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}},"outputId":"2a56368b-0e70-4b19-87b4-01d0cda84fd1"},"source":["rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n","rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","\n","start = time.time()\n","all_losses = []\n","test_losses = []\n","loss_avg = 0\n","test_loss_avg = 0\n","\n","\n","print(\"Training for %d epochs...\" % n_epochs)\n","for epoch in range(1, n_epochs + 1):\n","    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n","    loss_avg += loss\n","    \n","    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n","    test_loss_avg += test_loss\n","\n","    if epoch % print_every == 0:\n","        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n","        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n","\n","    if epoch % plot_every == 0:\n","        all_losses.append(loss_avg / plot_every)\n","        test_losses.append(test_loss_avg / plot_every)\n","        loss_avg = 0\n","        test_loss_avg = 0"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Training for 5000 epochs...\n","[0m 28s (50 1%) train loss: 2.0650, test_loss: 2.0951]\n","Whove to me doud Cise me pladg--wall sworgs.\n","Tou I hill dear wime coull leets ination thold havan it h \n","\n","[0m 50s (100 2%) train loss: 1.8523, test_loss: 1.9483]\n","Whiching mented hoof, theil for herely goet him Vaugh,\n","Het\n","You do this the consiss I dushis and minter \n","\n","[1m 12s (150 3%) train loss: 1.7664, test_loss: 1.8950]\n","What the stard and in the hand! shis coufter parrely the hath countring bet your lards.\n","\n","QUEEN BOKK:\n","M \n","\n","[1m 34s (200 4%) train loss: 1.7282, test_loss: 1.8671]\n","What in recend and\n","Dorther and your be mady of spome in sast when know the vait should it this extry f \n","\n","[1m 55s (250 5%) train loss: 1.7056, test_loss: 1.8427]\n","Who, what but resting thy have mesul the gose of stap you sont, for the wranly frown or a rack speak.\n"," \n","\n","[2m 17s (300 6%) train loss: 1.6643, test_loss: 1.8219]\n","What love 'en not think the prome\n","But thy poodfine the betery will, so refort valt they my horber this \n","\n","[2m 38s (350 7%) train loss: 1.6532, test_loss: 1.8071]\n","Which in to nouls of furses?\n","\n","WARWICK:\n","And Frow may doved with aland thy shall my hearts.\n","\n","PRINCE:\n","Who \n","\n","[3m 0s (400 8%) train loss: 1.6610, test_loss: 1.8056]\n","Wham the himblations it I werst wis drear.\n","\n","ROMEO:\n","What unster with have amoliors, by the o' sil, let  \n","\n","[3m 21s (450 9%) train loss: 1.6272, test_loss: 1.8104]\n","Where in fill newsal herrow well have he lade one.\n","\n","RUMINGARET:\n","The first the fauld let I have is not  \n","\n","[3m 43s (500 10%) train loss: 1.6299, test_loss: 1.7963]\n","Whouluning trow revile his take use make had God composhing pardant all but.\n","\n","BUCKINGHAM:\n","Do tave so,  \n","\n","[4m 5s (550 11%) train loss: 1.6270, test_loss: 1.7825]\n","Whore dound the pray?\n","\n","LUCENTIO:\n","Marge made in somion thou death?\n","\n","TYRGILIA:\n","Is the art servise hope w \n","\n","[4m 26s (600 12%) train loss: 1.5858, test_loss: 1.8121]\n","Whe thought 'tis not person me,\n","'He came is is generrawe\n","That perage think place:\n","And to that after ge \n","\n","[4m 48s (650 13%) train loss: 1.5756, test_loss: 1.8084]\n","What love art proight.\n","\n","Mare you; that that is shall could had not betwerry a litterey in there me tog \n","\n","[5m 10s (700 14%) train loss: 1.5987, test_loss: 1.7429]\n","Whoughter,\n","It prayver, do my shall as is the well,\n","And his ream; I will thou this so for faces serves. \n","\n","[5m 31s (750 15%) train loss: 1.5620, test_loss: 1.7609]\n","Why, he worthy tear,\n","I lustise of the bonaltent with my say the peace my did and rather short your car \n","\n","[5m 53s (800 16%) train loss: 1.6207, test_loss: 1.8404]\n","Whem thou take their pray, speably world thy son:\n","Thou hourst\n","Nor Richmant in Youd not now,\n","But not bu \n","\n","[6m 14s (850 17%) train loss: 1.6153, test_loss: 1.7989]\n","Where and the more I mest our more ambeed of go grace to leave my gear a mocking\n","And it from the chore \n","\n","[6m 36s (900 18%) train loss: 1.5779, test_loss: 1.7344]\n","Whe wan:\n","The go like you use.\n","\n","Glourt that's daughternest?\n","\n","DUKE VINCENTIO:\n","Was sold of thy lain that, \n","\n","[6m 58s (950 19%) train loss: 1.5725, test_loss: 1.7445]\n","Where know\n","say it shall me which me:\n","Have your fear so souls--was to beer was the Lorth Reak too do yo \n","\n","[7m 19s (1000 20%) train loss: 1.5841, test_loss: 1.7249]\n","Where new, not my rance the harth of Againce hie-dead, and be a here, and the knowns are trait nor com \n","\n","[7m 41s (1050 21%) train loss: 1.5845, test_loss: 1.8030]\n","Whough is my paint the person, are is quittation uncass: when the good son: whose two as the father lo \n","\n","[8m 3s (1100 22%) train loss: 1.5597, test_loss: 1.7484]\n","Where of will do a warrious heart they sway: more will this ragethers, she was go alike of their crom  \n","\n","[8m 24s (1150 23%) train loss: 1.5788, test_loss: 1.7536]\n","Whem to me of men to lived\n","I hath was our buttally Richard to hand, but tongue rich upon which I matte \n","\n","[8m 46s (1200 24%) train loss: 1.5520, test_loss: 1.8038]\n","What what thevings of victes?\n","Whid that you done an\n","I am will not blood I talk us mother and him: I sp \n","\n","[9m 7s (1250 25%) train loss: 1.5428, test_loss: 1.7912]\n","Wherein the flight wast Rome madies to thy husber'd, that their do and I desservaws love a grace,\n","My s \n","\n","[9m 29s (1300 26%) train loss: 1.5620, test_loss: 1.7690]\n","Where\n","That never still had that I thanks on, he rease;\n","Tell briate's see and of tsomer:\n","I have not dis \n","\n","[9m 51s (1350 27%) train loss: 1.5930, test_loss: 1.7271]\n","Whought of my fair purpurngum as night his must mouthouses the house to living with tuchion, and thy l \n","\n","[10m 12s (1400 28%) train loss: 1.5679, test_loss: 1.7065]\n","What here one but I limbrues of and my cape; a more were I stand you, and to gran's father!\n","Shall made \n","\n","[10m 34s (1450 28%) train loss: 1.5955, test_loss: 1.8051]\n","Why doth.\n","Your posent and in the word him thou art break our are non thy all the bedeed for all Nay, t \n","\n","[10m 55s (1500 30%) train loss: 1.5598, test_loss: 1.7940]\n","Whan worthy so was with as. Come of this was my husband.\n","\n","MARIANA:\n","More must though with in thy good k \n","\n","[11m 17s (1550 31%) train loss: 1.5680, test_loss: 1.7569]\n","Why, when this best where I know sounds too'd of them;\n","And two leave good more with my say the voints  \n","\n","[11m 39s (1600 32%) train loss: 1.5447, test_loss: 1.8069]\n","Whew ten not thy lord,\n","Better.\n","I will one this for at the who good prise,\n","As tire.\n","\n","RIVERS:\n","I'll so sh \n","\n","[12m 0s (1650 33%) train loss: 1.5256, test_loss: 1.7912]\n","Whan art one at damners\n","Withsel life of thy other's fortune for you I the lord but not we bear, is the \n","\n","[12m 22s (1700 34%) train loss: 1.5503, test_loss: 1.7476]\n","Whom the bangete great them me hath to heartices dream in my son sent, if the exceps thou should they  \n","\n","[12m 43s (1750 35%) train loss: 1.5641, test_loss: 1.7683]\n","Whe as you takerned in my brother she purses the sut hear you hath to his words to the desenge to all. \n","\n","[13m 5s (1800 36%) train loss: 1.5406, test_loss: 1.7557]\n","Whepo the majesty.\n","\n","GREMIO:\n","Why, he this she will him, but do he whom a told wisles, then God, like dr \n","\n","[13m 26s (1850 37%) train loss: 1.5387, test_loss: 1.7374]\n","Whap you, therefore, the fasces to the slixes in his deast?\n","\n","Merdio dongor the get to personerea I am  \n","\n","[13m 48s (1900 38%) train loss: 1.5586, test_loss: 1.7516]\n","Why cannot hath scaritest to lords of fall descence a each\n","With be\n","A ameet the blice a boot tongue to  \n","\n","[14m 9s (1950 39%) train loss: 1.5619, test_loss: 1.7734]\n","Whan serving.\n","\n","ISABELLA:\n","And which thine meply so father thee to chance, 'tis do a fals, which is then \n","\n","[14m 31s (2000 40%) train loss: 1.5718, test_loss: 1.7590]\n","Wher, be thou still the crown all he rohy, now, thou had as how to he honour of ere grace to your say. \n","\n","[14m 53s (2050 41%) train loss: 1.5734, test_loss: 1.7667]\n","Where breem,\n","A clatience thee not that's queen it is face child, thank the kind that dayme there you b \n","\n","[15m 14s (2100 42%) train loss: 1.5694, test_loss: 1.7916]\n","What the just.\n","\n","MENENIUS:\n","We deside the prove thee, I have besting waster,\n","The-vose these man a make y \n","\n","[15m 36s (2150 43%) train loss: 1.5780, test_loss: 1.7560]\n","What is a may any, hums so wife o' the genseen a rash you with, which is the lieget to not threath of  \n","\n","[15m 57s (2200 44%) train loss: 1.5309, test_loss: 1.7640]\n","Wher too many murdernest,\n","Let you, will never hear of chaloming of my mind true dispittle mock the mew \n","\n","[16m 19s (2250 45%) train loss: 1.5458, test_loss: 1.7551]\n","What he have what rour's such be faths you crown of him have old pluck,\n","That mindoms, let the hox,\n","How \n","\n","[16m 40s (2300 46%) train loss: 1.5625, test_loss: 1.7688]\n","Wher are farewed drawer like bring and say him you have now?\n","\n","JULIET:\n","Nor so not him.\n","My lord, for tha \n","\n","[17m 2s (2350 47%) train loss: 1.5396, test_loss: 1.7447]\n","Wher so heart, which hath expitiles all can sleeph monty their have tompers her bear-say Bolight to us \n","\n","[17m 23s (2400 48%) train loss: 1.5210, test_loss: 1.7510]\n","Where of tears here\n","Whither grace do that anach his war;\n","For he do.\n","what add the fearful me here of st \n","\n","[17m 45s (2450 49%) train loss: 1.5535, test_loss: 1.7533]\n","Wher brother make you have he malmed make the be the a mark--\n","Sirrous good heart and as be are may put \n","\n","[18m 6s (2500 50%) train loss: 1.5839, test_loss: 1.7639]\n","Why shall you how hold my sir, shall a pritheence? megn'd, no meet a man't, this dead, and what I will \n","\n","[18m 28s (2550 51%) train loss: 1.5484, test_loss: 1.7615]\n","Whey sun the power the stail:\n","And in do the heart and them the stations a even father maidish thee by  \n","\n","[18m 49s (2600 52%) train loss: 1.5788, test_loss: 1.7889]\n","Whether with crown,\n","That the live them, and Angelo, come to oet take are have but constens that in my  \n","\n","[19m 11s (2650 53%) train loss: 1.5336, test_loss: 1.7449]\n","Wher will hearts;\n","That more isse,\n","And for your lies the father lie?\n","\n","FRIAR LAURENCE:\n","Qothering\n","And but \n","\n","[19m 32s (2700 54%) train loss: 1.5197, test_loss: 1.7486]\n","Why more well,\n","And that if down nothing the head! will of my brath of heart shame it such brepoplege h \n","\n","[19m 54s (2750 55%) train loss: 1.5973, test_loss: 1.7408]\n","Whan nest.\n","What so archon wom the suffen on thee, and form the the love the pust,\n","Should it as the ble \n","\n","[20m 15s (2800 56%) train loss: 1.5484, test_loss: 1.7349]\n","What I she trinks with my toing worshig it shall but the sene the baning hath a commandre heart the lo \n","\n","[20m 37s (2850 56%) train loss: 1.5779, test_loss: 1.7797]\n","Why shall not many there wordre throne\n","To thie feeet of York? Think\n","A crown so had have duke, you let  \n","\n","[20m 58s (2900 57%) train loss: 1.5793, test_loss: 1.7999]\n","Wher from thy sun that heart is lack into the hand not a kind\n","Of the plead woman:\n","My such of his man w \n","\n","[21m 20s (2950 59%) train loss: 1.5771, test_loss: 1.7362]\n","Whethirn thee, and in the suck an noble love!\n","\n","CORIOLANUS:\n","He soul\n","And words in thy duke with an Lucio \n","\n","[21m 41s (3000 60%) train loss: 1.5631, test_loss: 1.7632]\n","Whan all conspiton your recear untwer: let the stark a kill gardical are be a man were a the daught\n","To \n","\n","[22m 3s (3050 61%) train loss: 1.5369, test_loss: 1.7597]\n","Whey from the dust have,\n","And the shall to go the lord send do?\n","\n","VOLUMNIAGNA:\n","Aive with is now made a b \n","\n","[22m 25s (3100 62%) train loss: 1.5528, test_loss: 1.7524]\n","Wher shall be couls thy save his shall that you the in your such in the noble Can a procedomy to his c \n","\n","[22m 46s (3150 63%) train loss: 1.5626, test_loss: 1.7300]\n","Who brother that if right:\n","Yesfice:\n","I carnate it.\n","\n","Seconvio, but the wife their commondon will with th \n","\n","[23m 8s (3200 64%) train loss: 1.5359, test_loss: 1.7553]\n","Whel have lives of, as that the peace; and doubt made now is some hand of the brands.\n","\n","LORD RICHARD II \n","\n","[23m 29s (3250 65%) train loss: 1.5645, test_loss: 1.7320]\n","Whereet;\n","And come soverve.\n","\n","CORIOLANUS:\n","The compele;\n","And worthy better and mare!\n","\n","CAMILLO:\n","And is the  \n","\n","[23m 51s (3300 66%) train loss: 1.5447, test_loss: 1.8214]\n","Wheche will'd be slasterous for thy discals,\n","And with this presence.\n","\n","Nurperal's Romeo, but, stand sha \n","\n","[24m 12s (3350 67%) train loss: 1.5502, test_loss: 1.7333]\n","Whan, besember\n","To many with thy lands\n","And randitiop,\n","To with my shall down by presens chargeth him, co \n","\n","[24m 34s (3400 68%) train loss: 1.5574, test_loss: 1.7847]\n","Whant,\n","Thou the servay's sun be like proit the grost.\n","\n","HENRY PERCY:\n","O mingly\n","A son, thou caporn him yo \n","\n","[24m 55s (3450 69%) train loss: 1.5422, test_loss: 1.7777]\n","Whan with now hards to but never now, my fail.\n","\n","Those a tracker:\n","Procest of with thy woo, speak on say \n","\n","[25m 17s (3500 70%) train loss: 1.5483, test_loss: 1.7959]\n","Whick in his kindle\n","What my paint her have bleasing off a with a world's not him dages, second these s \n","\n","[25m 40s (3550 71%) train loss: 1.5178, test_loss: 1.8315]\n","Whe false the poison.\n","\n","ESCALUS:\n","And,\n","With the falling cannot for their seems good than shall.\n","\n","SICINIU \n","\n","[26m 2s (3600 72%) train loss: 1.4956, test_loss: 1.7472]\n","Where, and one heart!\n","\n","ANGELO:\n","O sees of heaven counture thy thine to gen,\n","As is my father,\n","The part t \n","\n","[26m 24s (3650 73%) train loss: 1.5585, test_loss: 1.7592]\n","What have my fellow, if you gone not be licher to thy so good harm, and oh oritor,\n","With netch to the,  \n","\n","[26m 45s (3700 74%) train loss: 1.5347, test_loss: 1.7657]\n","What not love their must the plant of cannot is blood be into less ask the losa yanch'd of the loved a \n","\n","[27m 7s (3750 75%) train loss: 1.5135, test_loss: 1.7631]\n","What.\n","\n","MERCAUCEY:\n","So you shall be with my look so king of earth a son out\n","Althou are liege: there is h \n","\n","[27m 29s (3800 76%) train loss: 1.5354, test_loss: 1.8090]\n","What wash.\n","\n","MENENIUS:\n","My county like thee dream.\n","\n","Provost:\n","I have which hath seize of a said to marray \n","\n","[27m 51s (3850 77%) train loss: 1.5320, test_loss: 1.7558]\n","Why not he lords much,\n","Gotent to dead,\n","The same you win the ready see, for that is in you\n","I have this  \n","\n","[28m 12s (3900 78%) train loss: 1.5620, test_loss: 1.7813]\n","Wher chane far reasonesty most thy since o' the do canst mound of restine.\n","\n","EDWARD:\n","Ay,, he will befor \n","\n","[28m 34s (3950 79%) train loss: 1.5391, test_loss: 1.7598]\n","What, is the was on an restance.\n","Their noble adives as proclabst.\n","If a implages I musk a heavers\n","I hat \n","\n","[28m 55s (4000 80%) train loss: 1.5505, test_loss: 1.7768]\n","Whon distried in oether meet, there is thing\n","I do to all fean.\n","\n","LORD WILLOTH:\n","So should not perjust:\n","L \n","\n","[29m 17s (4050 81%) train loss: 1.5491, test_loss: 1.7539]\n","What were are from Marcius the impurn'd in wound, but in his noble will friend to do you this you to h \n","\n","[29m 39s (4100 82%) train loss: 1.5354, test_loss: 1.7645]\n","Whose me,\n","By the tord; my Edward, I some friends\n","She could have heart swordow the accuse, they noth an \n","\n","[30m 0s (4150 83%) train loss: 1.5452, test_loss: 1.7583]\n","What that for looks hate.\n","Caiced:\n","What, call to presence the streast worse:\n","His begiry procless come,  \n","\n","[30m 22s (4200 84%) train loss: 1.5378, test_loss: 1.7883]\n","Wherely is the will it in the called of may make when we now, sir? the baniss could here, I'll shall h \n","\n","[30m 43s (4250 85%) train loss: 1.5483, test_loss: 1.7560]\n","Where be\n","Turved so but to make him son thy catiles.\n","\n","Provost:\n","Ay, but is should fole:\n","Seringland.\n","\n","COR \n","\n","[31m 5s (4300 86%) train loss: 1.5382, test_loss: 1.7511]\n","What to one since their commanisest to the royal heart'st have the faith dowed and presues you took to \n","\n","[31m 26s (4350 87%) train loss: 1.5401, test_loss: 1.7937]\n","Wher, and my lady's depory appries, seize of states: or bitive, slavers, I woo, he see for my be consu \n","\n","[31m 48s (4400 88%) train loss: 1.5586, test_loss: 1.7549]\n","Whereed mehal's one your heart so wrine you our take her we were not be primy.\n","\n","GLOUCESTER:\n","He must no \n","\n","[32m 9s (4450 89%) train loss: 1.5497, test_loss: 1.7366]\n","What have many death, in Suning realing as he danger, the more prove mill, my see in Buckingnolio,\n","The \n","\n","[32m 31s (4500 90%) train loss: 1.5489, test_loss: 1.7367]\n","Whink to father at that was mine to death, much send:\n","It not doad and deer; the peace, unfaling deal y \n","\n","[32m 53s (4550 91%) train loss: 1.5405, test_loss: 1.7236]\n","Whey soldry foul ever that happice, if I would demption:\n","Give way are let him, young canscemberly to d \n","\n","[33m 14s (4600 92%) train loss: 1.5552, test_loss: 1.7883]\n","Whey my reedod,\n","That of the condead: and make my will sort thou streath,\n","But brother up the mattle in  \n","\n","[33m 36s (4650 93%) train loss: 1.5594, test_loss: 1.7365]\n","Where that coudourple me;\n","Let live me unto dies,\n","Which the come word:\n","When I will drawer-dot falls,\n","Wh \n","\n","[33m 57s (4700 94%) train loss: 1.5880, test_loss: 1.8124]\n","Whellow.\n","\n","KING LEWIS:\n","Say, the bectinurtable the grace; and best thou beat,\n","As my rich hark thou has s \n","\n","[34m 19s (4750 95%) train loss: 1.5196, test_loss: 1.8077]\n","Whear.\n","\n","JULIET:\n","O, a give a good child,\n","Here, Hereford.\n","\n","KING EDWARD IV:\n","My cannil. I have worth\n","Redde \n","\n","[34m 40s (4800 96%) train loss: 1.5369, test_loss: 1.7873]\n","Whis the hand of then did randermard, spilly heards of his alless\n","To betweed? 'The sent me on one is t \n","\n","[35m 2s (4850 97%) train loss: 1.5377, test_loss: 1.7902]\n","Whick the majest you, led the commisse, gest of a man?\n","\n","KING RICHARD III:\n","Livest and your puts a holy  \n","\n","[35m 24s (4900 98%) train loss: 1.5537, test_loss: 1.7391]\n","What time dangere\n","And bastire, why, the to they doth 'twere be chase in do tience, back of thy lost ma \n","\n","[35m 45s (4950 99%) train loss: 1.5560, test_loss: 1.7793]\n","Whyances.\n","Have him self searth with remain them to our torce kindle himself? he\n","love, go which soverei \n","\n","[36m 7s (5000 100%) train loss: 1.5636, test_loss: 1.7990]\n","Whwat the consenturd of him he\n","Where fair great 'twere worned, if it have be prince?\n","\n","BUSHY:\n","Her not n \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"rKSNGav48URf","executionInfo":{"status":"ok","timestamp":1619589555899,"user_tz":300,"elapsed":188,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}}},"source":["# save network\n","torch.save(rnn.state_dict(), './rnn_generator.pth')"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y6hqv3qf8URf"},"source":["# Plot the Training and Test Losses"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"collapsed":true,"id":"Ld8hvgKE8URf","executionInfo":{"status":"ok","timestamp":1619589558624,"user_tz":300,"elapsed":469,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}},"outputId":"4f87af77-7e8d-432e-db30-ad89c7e01e83"},"source":["import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","plt.figure()\n","plt.plot(all_losses)\n","plt.plot(test_losses, color='r')"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f514004ef10>]"]},"metadata":{"tags":[]},"execution_count":22},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"FSni_VLt8URg"},"source":["# Evaluate text generation\n","\n","Check what the outputted text looks like"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"pPIbhsbG8URg","executionInfo":{"status":"ok","timestamp":1619589561337,"user_tz":300,"elapsed":798,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}},"outputId":"61b28e02-382c-4397-8621-cad5ff7ae8d8"},"source":["print(evaluate(rnn, prime_str='Th', predict_len=1000))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Thank the own sing was eye and cannot son'st him stands, for plainty never a from my friar you, by the honeds he's light, cold shall be these laws,\n","To have offence\n","And shall night my Parory's ways whom youls for it, here unswey such she is have;\n","For I'll life to hour Hereford on his blood not have excred the dead was of this did you fill the cockey my offended to liencht the worsing of mine, we formed.\n","\n","BENVOLIO:\n","The more makes on, this now so cause's, grally all here your parding of my Lantertain'd in forgend these thrius, but you was be he leaves, but love him men that this lighted had not lost are be claid or excep, have well before hum!\n","And have his news.\n","\n","BUCKINGHAM:\n","By me thy headful hum;\n","Will lord, so I have have a life banishing for she all be child in stries is sees;\n","But be underses in my hat to the forton, the lave death, that her know me this may anyinght me but not here need?\n","\n","KING RICHARD III:\n","I prison?\n","\n","KING EDWARD IV:\n","Means have the can you.\n","\n","Lonce beat you.\n","\n","ARWICLARET:\n","W\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EfVEGvYe8URg"},"source":["# Hyperparameter Tuning\n","\n","Some things you should try to improve your network performance are:\n","- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n","- Try adding 1 or two more layers\n","- Increase the hidden layer size\n","- Changing the learning rate\n","\n","**TODO:** Try changing the RNN type and hyperparameters. Record your results."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"id":"ylyg_iE-8URg","executionInfo":{"status":"error","timestamp":1619593598071,"user_tz":300,"elapsed":2032230,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}},"outputId":"8ae417b6-ccad-492f-decf-8db7b3fcbf2b"},"source":["rnn = RNN(n_characters, hidden_size, n_characters, model_type=\"gru\", n_layers=2).to(device)\n","rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","\n","start = time.time()\n","all_losses = []\n","test_losses = []\n","loss_avg = 0\n","test_loss_avg = 0\n","\n","\n","print(\"Training for %d epochs...\" % n_epochs)\n","for epoch in range(1, n_epochs + 1):\n","    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n","    loss_avg += loss\n","    \n","    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n","    test_loss_avg += test_loss\n","\n","    if epoch % print_every == 0:\n","        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n","        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n","\n","    if epoch % plot_every == 0:\n","        all_losses.append(loss_avg / plot_every)\n","        test_losses.append(test_loss_avg / plot_every)\n","        loss_avg = 0\n","        test_loss_avg = 0"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Training for 5000 epochs...\n","[0m 24s (50 1%) train loss: 2.0572, test_loss: 2.0937]\n","Wheartrevers of to ared ktap, gear douth hit and greage with matte,\n","The you he tens mere, and; be hill \n","\n","[0m 49s (100 2%) train loss: 1.7930, test_loss: 1.8840]\n","Which nrangthere.\n","\n","DUKEN VINCENTIO:\n","I will the hear her, whill for stracks lame and suming bring,\n","Wher \n","\n","[1m 13s (150 3%) train loss: 1.6362, test_loss: 1.7862]\n","Whothift foul great him after not.\n","We heard! I lord nor earian and my from know'sh\n","My more but my arm  \n","\n","[1m 38s (200 4%) train loss: 1.6057, test_loss: 1.6976]\n","Why,\n","Eperselse of he had the prust? From have fortune\n","The sir, Clawelf thou there to she venger,\n","And c \n","\n","[2m 2s (250 5%) train loss: 1.5540, test_loss: 1.7570]\n","When are what!\n","Your slets every, I and I hass my lad here\n","As my heart a bemqueaul father country.\n","\n","Thi \n","\n","[2m 27s (300 6%) train loss: 1.5219, test_loss: 1.6923]\n","Wherity dien\n","That there to Harse falsed with for my leave\n","of the king the both height your carrance re \n","\n","[2m 51s (350 7%) train loss: 1.4536, test_loss: 1.6480]\n","What hath a cousin\n","As to their pernt husband and\n","As we marriest die, what this notter,\n","As yaver prove  \n","\n","[3m 15s (400 8%) train loss: 1.4514, test_loss: 1.6364]\n","Whst the queenest.\n","\n","JULIET:\n","So, sir, sir: seem are this is the scaning all fly:\n","Thou need thy mother s \n","\n","[3m 40s (450 9%) train loss: 1.4410, test_loss: 1.6704]\n","Where, less I have with their heads to play with knew, for a banished hours him,\n","And offle call deed w \n","\n","[4m 4s (500 10%) train loss: 1.3943, test_loss: 1.5839]\n","What he lovel'd a word,\n","As nea; I will be thince meld performaging keep me\n","Art you the patient;\n","And pl \n","\n","[4m 28s (550 11%) train loss: 1.4514, test_loss: 1.6620]\n","Whwatcement out the state me the our man farewell: I say at him beloved though that was the ground Ric \n","\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-2bc900487180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training for %d epochs...\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mload_random_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-8fb352d903c0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rnn, input, target, optimizer, criterion)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/assignment4_materials/assignment4_materials/rnn/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m##########       END      ##########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AwCo2Y6UFN9i","outputId":"a3848a50-201c-49aa-b0dd-54eaffea9f8d"},"source":["rnn = RNN(n_characters, hidden_size, n_characters, model_type=\"lstm\", n_layers=1).to(device)\n","rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","\n","start = time.time()\n","all_losses = []\n","test_losses = []\n","loss_avg = 0\n","test_loss_avg = 0\n","\n","\n","print(\"Training for %d epochs...\" % n_epochs)\n","for epoch in range(1, n_epochs + 1):\n","    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n","    loss_avg += loss\n","    \n","    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n","    test_loss_avg += test_loss\n","\n","    if epoch % print_every == 0:\n","        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n","        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n","\n","    if epoch % plot_every == 0:\n","        all_losses.append(loss_avg / plot_every)\n","        test_losses.append(test_loss_avg / plot_every)\n","        loss_avg = 0\n","        test_loss_avg = 0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training for 5000 epochs...\n","[0m 22s (50 1%) train loss: 2.0750, test_loss: 2.1123]\n","Wher the make shour whald you smadt, wist her in my lave foull som seat:\n","Am sie that, Ione to brome th \n","\n","[0m 45s (100 2%) train loss: 1.9011, test_loss: 1.9441]\n","Wherind men gound\n","Engerains,\n","The sonling shall the me in thy lave dear knisenvy was gown\n","Where the som \n","\n","[1m 7s (150 3%) train loss: 1.7445, test_loss: 1.8612]\n","Where, with the lothose too when there am you know'd, for gerate I heer the fave waids with thou can t \n","\n","[1m 29s (200 4%) train loss: 1.7025, test_loss: 1.8249]\n","Which her most wat him I to pight for not we am it; there would you gracese,\n","Which hose him thy lord,  \n","\n","[1m 52s (250 5%) train loss: 1.6440, test_loss: 1.7721]\n","What I apey strange before thou righ therefore you reath three that the great, and with graces and nob \n","\n","[2m 14s (300 6%) train loss: 1.6204, test_loss: 1.7773]\n","Where a some betite your brack meet the brother you the body. For him think and compay the mornous too \n","\n","[2m 37s (350 7%) train loss: 1.5908, test_loss: 1.7543]\n","When day to acts.\n","\n","ED Ast said a sound I says thought a part bear that soul not or eneman spich found  \n","\n","[2m 59s (400 8%) train loss: 1.5397, test_loss: 1.7226]\n","While it you then sent me:\n","To him not death over too?\n","\n","HASTINGS:\n","Sin Marcence:\n","I may the graining not  \n","\n","[3m 21s (450 9%) train loss: 1.5786, test_loss: 1.6784]\n","When friend: and persund\n","With you and thy sorroward and sward his isself and with I gracied dread, wit \n","\n","[3m 44s (500 10%) train loss: 1.5152, test_loss: 1.7126]\n","Where the goods a mans.\n","\n","Third field and canstle be beceims\n","Villaul come of my fear.\n","\n","GLOUCESTER:\n","How  \n","\n","[4m 6s (550 11%) train loss: 1.5355, test_loss: 1.7255]\n","Whose valiout steen of him; with stand,\n","Well and king go:\n","When shall be part on my father wronders,\n","Or \n","\n","[4m 28s (600 12%) train loss: 1.5136, test_loss: 1.7210]\n","What I should worth of the ears!\n","Can their upbil on you with seen's shall I cause.\n","\n","DUKE VINCENTIO:\n","Sh \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"64lpU65dWhAX","executionInfo":{"status":"aborted","timestamp":1619586083604,"user_tz":300,"elapsed":2228835,"user":{"displayName":"Quinn Collins","photoUrl":"","userId":"10272045741572569864"}}},"source":[""],"execution_count":null,"outputs":[]}]}